# bloodhound-json-splitter
Python script to split very large JSON files generated by SharpHound into smaller files.

In my experience, running SharpHound in very large Active Directory environments can generate very large corresponding JSON files (1gb+) with the domain information.  The Bloodhound GUI will often run out of available memory, regardless of the resources available, and error out when trying to parse and upload these files.

This script currently takes two arguments, the input file and the number of JSON objects to include in each output file.  It will parse through the input file and extract the provided number of objects, build a new collection of JSON objects matching the format Bloodhound expects, and write the information to a new file in the same directory in the format of output1.json, output2.json, etc.

It is currently hardcoded to only work with the users file generated by Sharphound as that is usually the largest, but can be easily changed for the other output formats.

![image](https://user-images.githubusercontent.com/58894272/113525279-d6558b80-9568-11eb-9a3e-c683124b25be.png)
